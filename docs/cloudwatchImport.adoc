== AWS Cloudwatch LogGroup event export 

There are three ways to export logs from a log group to an S3 bucket/SIEM/Data Lake or any other destinations.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html[AWS Documentation]

* Subscription filter with Kinesis Data Streams
* Subscription filter with Amazon Data Firehose
* Subscription filter with AWS Lambda

Fluency Platform support all three methods. 

=== Subscription filter with Amazon Data Firehose

* Step 1: Setup IAM Roles for Cloudwatch and Firehose service. 
** Create a S3 bucket to hold the loggroup data. (shared by all loggroups)
** Create a new IAM role, allow Firehose to write to S3 bucket.
** Create a new IAM role, allow Cloudwatch to write to the Firehose stream.
** Create a new IAM user for Fluency Platform. Allow Fluency to read from the S3 bucket.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchFirehose.yaml[Cloudformation Template]
** Cloudformation Parameters: 
*** CloudWatchRole:  fluencyCloudwatchToFireHose 
*** FirehoseRole: fluencyFireHoseToS3
*** S3Bucket: {yourcompany}-fluency-cloudwatch-firehose
*** IAMUserForFluency: fluency-s3-read

* Step 2: Add Loggroup one by one 
** Create a new Firehose stream.
** Create a new subscription filter for the loggroup, set the destination to the Firehose stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchSubscriptionFilter.yaml[Cloudformation Template]
** Cloudformation Parameters:
*** CloudWatchRole:  fluencyCloudwatchToFireHose 
*** FilterName: passthrough
*** FilterPattern: ""
*** FirehoseRole: fluencyFireHoseToS3
*** LogGroup:  
*** S3Bucket: {yourcompany}-fluency-cloudwatch-firehose
*** Name:    //S3 object Prefix

* Pros:
** Easy to setup
** Cheaper than Kinesis data stream
* Cons:
** One firehose for one destination
** Limited destination options

=== Subscription filter with Kinesis Data Streams

* Step 1: Setup IAM Roles for Cloudwatch and Kinesis service.
** Create a new Kinesis stream.
** Create a new IAM role, allow Cloudwatch to write to the Kinesis stream.
** Create a new subscription filter for the loggroup, set the destination to the Kinesis stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchKinesis.yaml[Cloudformation Template]
** Cloudformation Parameters:
*** CloudWatchRole: fluencyCloudwatchToKinesis
*** FilterName: passToKinesis 
*** StreamName
*** LogGroup

* Step 2: Setup IAM User for Fluency Platform
** Allow this IAM user to read from the Kinesis stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyPlatformKinesisRead.yaml[Cloudformation Template]
** Cloudformation Parameters:
*** KinesisStream
*** IAMUser

* Pros:
** Easy to setup
** Flexible destination options
** 24 hours data retention
* Cons:
** More expensive (each stream cost $0.10 per hour plus data ingress cost)

=== Subscription filter with Lambda

* Create a S3 bucket for Lambda function to write to
* Create a IAM role for Lambda function (execution role).
** Allow Lambda to write to the S3 bucket
** Allow Lambda to write to Cloudwatch logs (if logging is needed) 
* Create a Lambda function
----
import datetime
import os
import uuid 
import base64
import boto3
import time

def lambda_handler(event, context):
    
    LOGGROUP = os.environ['LOGGROUP']
    DESTINATION_BUCKET = os.environ['BUCKET']
    PREFIX = os.environ['PREFIX']
    currentTime = datetime.datetime.now()
    LOGGROUP = LOGGROUP.replace('/','_')
    OBJECT_PREFIX = os.path.join(PREFIX, LOGGROUP, currentTime.strftime('%Y%m%d').format(os.path.sep))
    encoded_zipped_data = event['awslogs']['data']
    zipped_data = base64.b64decode(encoded_zipped_data)
    
    basename = currentTime.strftime('%H-%M-%S')
    object_path = '{}/{}-{}.json.gz'.format(OBJECT_PREFIX, currentTime.strftime('%H-%M-%S'),uuid.uuid1())
    s3 = boto3.client('s3') 
    s3.put_object(Body=zipped_data, Bucket=DESTINATION_BUCKET, Key=object_path)
----
* Attach a resource permission policy to this lambda, allow Cloudwatch loggroup to invoke this lambda function.
* Create a new subscription filter for the loggroup, set the destination to the Lambda function.
* Create IAM User for Fluency Platform. Allow Fluency to read from the S3 bucket.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchLambdaS3.yaml[Cloudformation Template]
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchLambdaS3WithLogging.yaml[Cloudformation Template with logging support]
** Cloudformation Parameters:
*** S3Bucket
*** S3ObjectPrefix
*** LambdaRole
*** LambdaFunctionName
*** LogGroup
*** FilterPattern
*** FilterName
*** IAMUserForFluency


* Pros:
** Flexible destination options
** Cheapest solution. Only pay lambda invocation cost
* Cons:
** More complex setup