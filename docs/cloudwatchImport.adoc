== AWS Cloudwatch LogGroup event export 

There are three ways to export logs from a log group to an S3 bucket/SIEM/Data Lake or any other destinations.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html[AWS Documentation]

* Subscription filter with Kinesis Data Streams
* Subscription filter with Amazon Data Firehose
* Subscription filter with AWS Lambda

Fluency Platform support all three methods. 

=== Subscription filter with Amazon Data Firehose

* Step 1: Setup IAM Roles for Cloudwatch and Firehose service. 
** Create a S3 bucket to hold the loggroup data. (shared by all loggroups)
** Create a new IAM role, allow Firehose to write to S3 bucket.
** Create a new IAM role, allow Cloudwatch to write to the Firehose stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchFirehose.yaml[Cloudformation Template]
** Cloudformation Parameters: 
*** CloudWatchRole:  fluencyCloudwatchToFireHose 
*** FirehoseRole: fluencyFireHoseToS3
*** S3Bucket: {yourcompany}-fluency-cloudwatch-firehose

* Step 2: Add Loggroup one by one 
** Create a new Firehose stream.
** Create a new subscription filter for the loggroup, set the destination to the Firehose stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchSubscriptionFilter.yaml[Cloudformation Template]
** Cloudformation Parameters:
*** CloudWatchRole:  fluencyCloudwatchToFireHose 
*** FilterName: passthrough
*** FilterPattern: ""
*** FirehoseRole: fluencyFireHoseToS3
*** LogGroup:  
*** S3Bucket: {yourcompany}-fluency-cloudwatch-firehose
*** Name:    //S3 object Prefix

* Step 3: Configure a "S3 bucket" integration in Fluency. 
** Set the bucket and region
** Set the mode to "read"
** Select the AWS authentication method: EC2 Instance Role, IAM User or Access Key (create a new IAM user)


* Step 4: Add a "S3" type data source in Fluency.  
** Set the Receiver to "CloudwatchEventSplit", input set to "JSON"
----
function main({obj, size, source, props}) {
    if (obj.messageType != "DATA_MESSAGE") {
      return null
    }
    let list = []
    let logEvents = obj.logEvents
    if (!logEvents) {
       printf("logEvents field not found")
       return null
    }
    for i, event = range logEvents {
      let envelop = {
         logGroup: obj.logGroup,
         logStream: obj.logStream,
         subscriptionFilters: obj.subscriptionFilters,
         "@message": event.message,
         "@type": "event",
         "@timestamp": event.timestamp,
         "@source": obj.logStream
      }
      list = append(list, envelop)
    }
    return list
}
----

* Pros:
** Easy to setup
** Cheaper than Kinesis data stream
* Cons:
** One firehose for one destination
** Limited destination options

=== Subscription filter with Kinesis Data Streams

* Step 1: Setup IAM Roles for Cloudwatch and Kinesis service.
** Create a new Kinesis stream.
** Create a new IAM role, allow Cloudwatch to write to the Kinesis stream.
** Create a new subscription filter for the loggroup, set the destination to the Kinesis stream.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchKinesis.yaml[Cloudformation Template]
** Cloudformation Parameters:
*** CloudWatchRole: fluencyCloudwatchToKinesis
*** FilterName: passToKinesis 
*** StreamName
*** LogGroup

* Step 2: Configure a "AWS Kinesis" integration in Fluency. 
** Set the kinesis stream name, region
** Set the mode to "read"
** Select the AWS authentication method: EC2 Instance Role, IAM User or Access Key (create a new IAM user)

* Step 3: Add a "Kinesis" type data source in Fluency.  
** Set the Receiver to "CloudwatchEventSplit", input set to "JSON"


* Pros:
** Easy to setup
** Flexible destination options
** 24 hours data retention
* Cons:
** More expensive (each stream cost $0.10 per hour plus data ingress cost)

=== Subscription filter with Lambda

* Create a S3 bucket for Lambda function to write to
* Create a IAM role for Lambda function (execution role).
** Allow Lambda to write to the S3 bucket
** Allow Lambda to write to Cloudwatch logs (if logging is needed) 
* Create a Lambda function
----
import datetime
import os
import uuid 
import base64
import boto3
import time

def lambda_handler(event, context):
    
    LOGGROUP = os.environ['LOGGROUP']
    DESTINATION_BUCKET = os.environ['BUCKET']
    PREFIX = os.environ['PREFIX']
    currentTime = datetime.datetime.now()
    LOGGROUP = LOGGROUP.replace('/','_')
    OBJECT_PREFIX = os.path.join(PREFIX, LOGGROUP, currentTime.strftime('%Y%m%d').format(os.path.sep))
    encoded_zipped_data = event['awslogs']['data']
    zipped_data = base64.b64decode(encoded_zipped_data)
    
    basename = currentTime.strftime('%H-%M-%S')
    object_path = '{}/{}-{}.json.gz'.format(OBJECT_PREFIX, currentTime.strftime('%H-%M-%S'),uuid.uuid1())
    s3 = boto3.client('s3') 
    s3.put_object(Body=zipped_data, Bucket=DESTINATION_BUCKET, Key=object_path)
----
* Attach a resource permission policy to this lambda, allow Cloudwatch loggroup to invoke this lambda function.
* Create a new subscription filter for the loggroup, set the destination to the Lambda function.
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchLambdaS3.yaml[Cloudformation Template]
** https://fluency-cloudformation.s3.us-east-2.amazonaws.com/FluencyCloudWatchLambdaS3WithLogging.yaml[Cloudformation Template with logging support]
** Cloudformation Parameters:
*** S3Bucket
*** S3ObjectPrefix
*** LambdaRole
*** LambdaFunctionName
*** LogGroup
*** FilterPattern
*** FilterName

* Step 2: Configure a "S3 bucket" integration in Fluency. 
** Set the bucket and region
** Set the mode to "read"
** Select the AWS authentication method: EC2 Instance Role, IAM User or Access Key (create a new IAM user)

* Step 3: Add a "S3" type data source in Fluency.  
** Set the Receiver to "CloudwatchEventSplit", input set to "JSON"


* Pros:
** Flexible destination options
** Cheapest solution. Only pay lambda invocation cost
* Cons:
** More complex setup